# AI Plugin Collection [Deutsch]

![Banner (created by ChatGPT)](<pictures/Banner (created by ChatGPT).png>)

Ein umfassendes Python-Plugin zur Verwaltung und Steuerung von Ollama Ã¼ber API und CLI. Dieses Plugin bietet eine einheitliche Schnittstelle fÃ¼r die Modellverwaltung, Service-Steuerung und KI-Generierung.

## ğŸŒ Sprache / Language

- ğŸ‡¬ğŸ‡§ [English](../README.md)
- ğŸ‡©ğŸ‡ª Deutsch (diese Datei)

<br>

## âš¡ Wichtigsten Merkmale

- **Dual Backend:** Nahtloser Wechsel zwischen API- oder CLI-BefehlsausfÃ¼hrung
- **Unified Manager:** Eine Schnittstelle (`OllamaManager`) zur Steuerung aller Subsysteme
- **Model Management:** Modelle auflisten, abrufen, lÃ¶schen und Ã¼berprÃ¼fen
- **AI Generation:** Textgenerierung (gestreamt oder zwischengespeichert)
- **Service Control:** Starten/ stoppen und DurchfÃ¼hren von SystemzustandsprÃ¼fungen
- **Cache Integration:** Optionale Caching-Ebene fÃ¼r wiederholte Eingabeaufforderungen
- **Conversation Management:** Aufrechterhaltung kontextbezogener Chat-Sitzungen
- **Helper Tools:** Plattforminstallation, Modellvalidierung, Token-SchÃ¤tzung
- **Config Loader:** Laden von Einstellungen direkt aus JSON-Konfigurationsdateien
- **Crossâ€‘Platform:** Funktioniert unter MacOS, Linux und Windows

<br>

## ğŸ“š Inhaltsverzeichnis

1. [Installation](#-installation)
    - [Prerequisites](#-prerequisites)
    - [Install Dependencies](#-install-dependencies)
2. [Quick Start](#-quick-start)
3. [Usage](#-usage)
   - [Setup & Configuration](#ï¸-setup--configuration)
   - [Model Management](#-model-management)
   - [Running Models](#-running-models)
   - [AI Generation](#-ai-generation)
   - [Conversation Example](#-conversation-example)
   - [Service Management](#-service-management)
   - [Helper Tools](#ï¸-helper-tools)
   - [Cache](#-cache)
   - [Backend Control](#-backend-control)
4. [Architecture Overview](#-architecture-overview)
5. [Backend Comparison](#ï¸-backend-comparison)
6. [Best Practices](#-best-practices)
7. [License](#-license)
8. [Contributors](#-contributors)
9. [Troubleshooting](#-troubleshooting)
10. [Supported Platforms](#-supported-platforms)

<hr><br>

## ğŸ§© Installation

### ğŸ§± Voraussetzungen

- Python 3.8+
- Ollama muss installiert sein (oder verwende die Hilfsfunktionen fÃ¼r die Installation)

### ğŸ“¦ AbhÃ¤ngigkeiten installieren

Lokal oder in einemÂ virtuellenÂ Environment:

```bash
pip install -r requirements.txt
```

Oder direkt:

```bash
pip install psutil requests pytest pytestâ€‘cov
```

Erforderliche Pakete:

| Package    | Zweck / Beschreibung                                                 |
| ---------- | -------------------------------------------------------------------- |
| psutil     | Prozessâ€‘ und Systemâ€‘Management (z.â€¯B. zur PrÃ¼fung, ob Ollama lÃ¤uft) |
| requests   | HTTPâ€‘Client zum Aufrufen der Ollamaâ€‘API                             |
| pytest     | Framework fÃ¼r Unitâ€‘ und Integrationâ€‘Tests                           |
| pytestâ€‘cov | Erweiterung zur Messung der Testâ€‘Coverage                           |

### ğŸ§ªâ€¯Run Tests

FÃ¼r vollstÃ¤ndige Tests (inklusive Decorators, Caching, HelpersÂ etc.):

```bash
pytest -v
```

FÃ¼hren einen bestimmten Test aus:
```bash
pytest tests/test_service_manager.py
```

mit **Coverageâ€‘Report**:

```bash
pytest --cov=src --cov-report=term-missing
```

Optionale Detailberichte (HTML imâ€¯`htmlcov/`â€‘Ordner):

```bash
pytest --cov=src --cov-report=html
open htmlcov/index.html  # oder `start htmlcov/index.html` unter Windows
```

<hr><br>

## ğŸš€ Schnellstart

Siehe auch [Beispieldatei](src/main.py) fÃ¼r eine vollstÃ¤ndige VorfÃ¼hrung.

```py
from modules.plugin_manager import OllamaManager

# Initialize with API backend (default)
manager = OllamaManager()

# Run health check
manager.health_check()

# List models
models = manager.list_models()

# Generate text
manager.generate("llama3.2:3b", "Explain objectâ€‘oriented programming.")
```

<hr><br>

## ğŸ’¡ Verwendung

### âš™ï¸ Einrichtung & Konfiguration

#### ğŸ—‚ï¸ Aus JSON-Konfiguration

```py
manager = OllamaManager.from_config_file("config.json", backend=OllamaBackend.API)
```

Beispiel fÃ¼r eine Konfigurationsdatei:

```json
{
  "host": "localhost",
  "port": 11434,
  "default_model": "llama3.2:3b"
}
```

<br>

### ğŸ¤– Model Management

```py
# List models
manager.list_models()

# Detailed list
manager.list_models_detailed()

# Model info
manager.model_info("llama3.2:3b")

# Pull and delete
manager.pull_model("llama3.2:3b")
manager.delete_model("llama3.2:3b")
```

<br>

### ğŸŸ¢ AusfÃ¼hren der Modelle

```py
# Show running models
manager.list_running_models()

# Start / stop model
manager.start_model("llama3.2:3b")
manager.stop_model("llama3.2:3b")

# Refresh lists
manager.refresh_running_models()
```

<br>

### ğŸ§  AI-Generation

```py
# Generate once
manager.generate("gemma3:4b", "Explain Python.")

# Cached call (faster second time)
manager.generate("llama3.2:3b", "Explain Python classes.", use_cache=True)

# Stream output
manager.generate_stream("llama3.2:3b", "Write a short poem about programming.")
```

<br>

### ğŸ’¬ Beispiel fÃ¼r eine AI-Konversation

```py
conv = manager.start_conversation(
    "llama3.2:3b",
    system_message="You are a friendly assistant."
)
manager.chat(conv, "What is recursion?")
manager.chat(conv, "Give me an example in Python.")
```

<br>

### ğŸ§© Service Management

```py
# Get Ollama version
print(manager.get_version())

# Check operating system
print(manager.get_operating_system())

# Check status
is_installed = manager.is_installed()
print(is_installed)
is_running = manager.is_process_active()
print(is_running)
api_ready = manager.get_api_status()
print(api_ready)

# Installation path
path = manager.get_installation_path()
print(path)

# Start/stop service
manager.start_service()
manager.stop_service()

# Health check
status = manager.health_check()
print(status)
```

<br>

### ğŸ› ï¸ Werkzeuge

```py
manager.validate_model_name("llama3.2:3b")
manager.estimate_tokens("Sample prompt text")

# MacOS installation
manager.check_homebrew_installed()
manager.try_installing_homebrew()
manager.install_on_macos()

# Linux installation
manager.try_installing_curl()
manager.install_on_linux()

# Windows installation
manager.check_winget_installed()
manager.check_chocolatey_installed()
manager.try_installing_winget()
manager.try_installing_choco()
manager.try_installing_direct_on_windows_only()
manager.install_on_windows()

# Show manual instructions
manager.show_manual_installation_instruction()
```

<br>

### ğŸ§® Cache

```py
manager.cache_stats()
manager.clear_expired_cache()
manager.export_cache_info()
```

<br>

### ğŸ”€ Backend Steuerung

```py
# Switch between API and CMD
manager.switch_backend(mode=OllamaBackend.API)
manager.switch_backend(mode=OllamaBackend.CMD)
print(manager.get_backend_type())
```

<hr><br>

## ğŸ§­ ArchitekturÃ¼bersicht

### ğŸ§© Module
```bash
modules/
â”œâ”€â”€ api_manager.py                  # REST-API-Interaktionen
â”œâ”€â”€ cmd_manager.py                  # CLI-Befehle
â”œâ”€â”€ conversation_manager.py         # Kontextuelle Konversation
â”œâ”€â”€ plugin_manager.py               # Einheitlicher OllamaManager
â””â”€â”€ service_manager.py              # Prozess- und Service-Kontrolle
```

### ğŸ§  Core
```bash
modules/
â”œâ”€â”€ cache_manager.py                # Cache-System
â”œâ”€â”€ decorators.py                   # Dekoratoren & Validierung
â””â”€â”€ helpers.py                      # Betriebssystem-Dienstprogramme und Installationsprogramme
```

### âš™ï¸ Config
```bash
modules/
â””â”€â”€ settings.py                     # Alle verÃ¤nderbaren Einstellungen
```

### âš™ï¸ Tests
```bash
tests/
â”œâ”€â”€ test_api_manager.py             # Tests fÃ¼r REST-API-Aufrufe
â”œâ”€â”€ test_cache_manager.py           # PrÃ¼ft Cache-System und SQLite-Logik
â”œâ”€â”€ test_cmd_manager.py             # CLI-Befehle und Parameter-Parsing
â”œâ”€â”€ test_conversation_manager.py    # Kontextverwaltung und GesprÃ¤chsfluss
â”œâ”€â”€ test_decorators.py              # Alle Dekorator- und Validierungsfunktionen
â”œâ”€â”€ test_helpers.py                 # Installationsroutinen, Brew/Winget u.a.
â”œâ”€â”€ test_plugin_manager.py          # Integration von OllamaManager / Plugins
â”œâ”€â”€ test_service_manager.py         # Prozesskontrolle und StatusprÃ¼fung
â””â”€â”€ test_settings.py                # Konfiguration und JSON-Ladefunktionen
```

<hr><br>

Das Plugin besteht aus mehreren spezialisierten Komponenten:

- **OllamaManager:** Einheitliche Schnittstelle, die alle Module in einer Fassade vereint
- **OllamaAPIManager:** REST-API-Kommunikation Ã¼ber:
  `/api/tags`, `/api/show`, `/api/pull`, `/api/delete`, `/api/generate`, `/api/ps`
- **OllamaCMDManager:** BefehlszeilenausfÃ¼hrung:
  `ollama list`, `ollama show`, `ollama pull`, `ollama rm`, `ollama run`, `ollama ps`, `ollama stop`
- **OllamaService:** Verwaltung des Lebenszyklus von Diensten und Ãœberwachung des Zustands
- **OllamaHelper:** Plattform-Installationsprogramme, Validierung und Hilfsfunktionen

<hr><br>

## âš–ï¸ Backend-Vergleich

| Funktion | API-Backend | CMD-Backend |
|---------|------------|-------------|
| Leistung | âš¡ Schneller | â±ï¸ Langsamer |
| Streaming | âœ… Nativ | âœ… Zeilenbasiert |
| Fortschrittsinformationen | âœ… Detailliert | âš ï¸ Parsing erforderlich |
| Parametersteuerung | âœ… VollstÃ¤ndig | âŒ EingeschrÃ¤nkt |
| AbhÃ¤ngigkeiten | requests | subprocess |

<hr><br>

## ğŸ’ Best Practices

- Verwende **API-Backend** fÃ¼r Produktions- oder asynchrone Anwendungen
- Verwende **CMD-Backend**, wenn die lokale API nicht erreichbar ist
- FÃ¼hre `health_check()` vor kritischen VorgÃ¤ngen aus
- Speichere wiederholte Generierungen im Cache, um die Leistung zu verbessern
- Behalte wÃ¤hrend der Entwicklung `verbose=True` fÃ¼r Protokolle und Ausdrucke bei

<hr><br>

## ğŸ“œ Lizenz

Dieses Projekt ist Open Source. Siehe [Lizenz](../LICENSE).

<hr><br>

## ğŸ™Œ Mitwirkende

![created-by](pictures/created-by.svg)

<hr><br>

## ğŸ§¯ Fehlerbehebung

### ğŸ§° Entwicklungsinstallation

Bei Problemen beim Importieren (zB. Module nicht gefunden), versuche:

```bash
pip install -e .
```

Dadurch wird das Projekt im **bearbeitbaren Modus** installiert und mit dem lokalen Ordner â€src/â€œ verknÃ¼pft, sodass Ã„nderungen sofort wirksam werden.

Dann kann direkt importiert werden:
```py
from modules.service_manager import Service
```

<hr><br>

## ğŸ’» UnterstÃ¼tzte Plattformen

- âœ… macOSÂ (Intelâ€¯&â€¯Appleâ€¯Silicon)
- âœ… LinuxÂ (Ubuntu, Debian, Fedoraâ€¦)
- âœ… WindowsÂ 10â€¯/â€¯11
