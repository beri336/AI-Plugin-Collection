# AI Plugin Collection [English]

![Banner (created by ChatGPT)](<docs/pictures/Banner (created by ChatGPT).png>)

A comprehensive Python plugin for managing and controlling Ollama via API and CLI. This plugin provides a unified interface for model management, service control, and AI generation.

## ğŸŒ Language / Sprache

- ğŸ‡¬ğŸ‡§ English (this file)
- ğŸ‡©ğŸ‡ª [Deutsch](docs/README.de.md)

<br>

## âš¡ Key Features

- **Dual Backend:** Seamless switch between API or CLI command execution
- **Unified Manager:** One interface (`OllamaManager`) controlling all subsystems
- **Model Management:** List, pull, delete and inspect models
- **AI Generation:** Text generation (streamed or cached)
- **Service Control:** Start/stop and perform system health checks
- **Cache Integration:** Optional caching layer for repeated prompts
- **Conversation Management:** Maintain contextual chat sessions
- **Helper Tools:** Platform installation, model validation, token estimation
- **Config Loader:** Load settings directly from JSON config files
- **Crossâ€‘Platform:** Works on macOS, Linux and Windows

<br>

## ğŸ“š Table of Contents

1. [Installation](#-installation)
    - [Prerequisites](#-prerequisites)
    - [Install Dependencies](#-install-dependencies)
2. [Quick Start](#-quick-start)
3. [Usage](#-usage)
   - [Setup & Configuration](#ï¸-setup--configuration)
   - [Model Management](#-model-management)
   - [Running Models](#-running-models)
   - [AI Generation](#-ai-generation)
   - [Conversation Example](#-conversation-example)
   - [Service Management](#-service-management)
   - [Helper Tools](#ï¸-helper-tools)
   - [Cache](#-cache)
   - [Backend Control](#-backend-control)
4. [Architecture Overview](#-architecture-overview)
5. [Backend Comparison](#ï¸-backend-comparison)
6. [Best Practices](#-best-practices)
7. [License](#-license)
8. [Contributors](#-contributors)
9. [Troubleshooting](#-troubleshooting)
10. [Supported Platforms](#-supported-platforms)

<hr><br>

## ğŸ§© Installation

### ğŸ§± Prerequisites

- Python 3.8+
- Ollama must be installed (or use the helper functions for installation)

### ğŸ“¦ Install Dependencies

Locally or in a virtual environment:

```bash
pip install -r requirements.txt
```

Or directly:

```bash
pip install psutil requests pytest pytest-cov
```

Required packages:

| Package    | Purpose / Description                                                 |
| ---------- | --------------------------------------------------------------------  |
| psutil     | Process and system management (e.g., to check whether Ollama is running) |
| requests   | HTTP client for calling the Ollama API                             |
| pytest     | Framework for unit and integration testing                           |
| pytestâ€‘cov | Extension for measuring test coverage                           |

### ğŸ§ªâ€¯Run Tests

For complete tests (including decorators, caching, helpers, etc.):

```bash
pytest -v
```

Run a specific test:
```bash
pytest tests/test_service_manager.py
```

With **coverage report**:

```bash
pytest --cov=src --cov-report=term-missing
```

Optional detailed reports (HTML in the `htmlcov/` folder):

```bash
pytest --cov=src --cov-report=html
open htmlcov/index.html  # or `start htmlcov/index.html` on Windows
```

<hr><br>

## ğŸš€ Quick Start

See also [example file](src/main.py) for a full demonstration.

```py
from modules.plugin_manager import OllamaManager

# Initialize with API backend (default)
manager = OllamaManager()

# Run health check
manager.health_check()

# List models
models = manager.list_models()

# Generate text
manager.generate("llama3.2:3b", "Explain objectâ€‘oriented programming.")
```

<hr><br>

## ğŸ’¡ Usage

### âš™ï¸ Setup & Configuration

#### ğŸ—‚ï¸ From JSON Config

```py
manager = OllamaManager.from_config_file("config.json", backend=OllamaBackend.API)
```

Config file example:

```json
{
  "host": "localhost",
  "port": 11434,
  "default_model": "llama3.2:3b"
}
```

<br>

### ğŸ¤– Model Management

```py
# List models
manager.list_models()

# Detailed list
manager.list_models_detailed()

# Model info
manager.model_info("llama3.2:3b")

# Pull and delete
manager.pull_model("llama3.2:3b")
manager.delete_model("llama3.2:3b")
```

<br>

### ğŸŸ¢ Running Models

```py
# Show running models
manager.list_running_models()

# Start / stop model
manager.start_model("llama3.2:3b")
manager.stop_model("llama3.2:3b")

# Refresh lists
manager.refresh_running_models()
```

<br>

### ğŸ§  AI Generation

```py
# Generate once
manager.generate("gemma3:4b", "Explain Python.")

# Cached call (faster second time)
manager.generate("llama3.2:3b", "Explain Python classes.", use_cache=True)

# Stream output
manager.generate_stream("llama3.2:3b", "Write a short poem about programming.")
```

<br>

### ğŸ’¬ Conversation Example

```py
conv = manager.start_conversation(
    "llama3.2:3b",
    system_message="You are a friendly assistant."
)
manager.chat(conv, "What is recursion?")
manager.chat(conv, "Give me an example in Python.")
```

<br>

### ğŸ§© Service Management

```py
# Get Ollama version
print(manager.get_version())

# Check operating system
print(manager.get_operating_system())

# Check status
is_installed = manager.is_installed()
print(is_installed)
is_running = manager.is_process_active()
print(is_running)
api_ready = manager.get_api_status()
print(api_ready)

# Installation path
path = manager.get_installation_path()
print(path)

# Start/stop service
manager.start_service()
manager.stop_service()

# Health check
status = manager.health_check()
print(status)
```

<br>

### ğŸ› ï¸ Helper Tools

```py
manager.validate_model_name("llama3.2:3b")
manager.estimate_tokens("Sample prompt text")

# macOS installation
manager.check_homebrew_installed()
manager.try_installing_homebrew()
manager.install_on_macos()

# Linux installation
manager.try_installing_curl()
manager.install_on_linux()

# Windows installation
manager.check_winget_installed()
manager.check_chocolatey_installed()
manager.try_installing_winget()
manager.try_installing_choco()
manager.try_installing_direct_on_windows_only()
manager.install_on_windows()

# Show manual instructions
manager.show_manual_installation_instruction()
```

<br>

### ğŸ§® Cache

```py
manager.cache_stats()
manager.clear_expired_cache()
manager.export_cache_info()
```

<br>

### ğŸ”€ Backend Control

```py
# Switch between API and CMD
manager.switch_backend(mode=OllamaBackend.API)
manager.switch_backend(mode=OllamaBackend.CMD)
print(manager.get_backend_type())
```

<hr><br>

## ğŸ§­ Architecture Overview

### ğŸ§© Module
```bash
modules/
â”œâ”€â”€ api_manager.py              # REST API interactions
â”œâ”€â”€ cmd_manager.py              # CLI commands
â”œâ”€â”€ conversation_manager.py     # Contextual conversation system
â”œâ”€â”€ plugin_manager.py           # Unified OllamaManager facade
â””â”€â”€ service_manager.py          # Process & service control
```

### ğŸ§  Core
```bash
modules/
â”œâ”€â”€ cache_manager.py            # Cache system
â”œâ”€â”€ decorators.py               # Decorators & validation
â””â”€â”€ helpers.py                  # OS utilities & installers
```

### âš™ï¸ Config
```bash
modules/
â””â”€â”€ settings.py                 # All modifiable settings
```

### âš™ï¸ Tests
```bash
tests/
â”œâ”€â”€ test_api_manager.py             # Tests for REST API calls
â”œâ”€â”€ test_cache_manager.py           # Checks cache system and SQLite logic
â”œâ”€â”€ test_cmd_manager.py             # CLI commands and parameter parsing
â”œâ”€â”€ test_conversation_manager.py    # Context management and conversation flow
â”œâ”€â”€ test_decorators.py              # All decorator and validation functions
â”œâ”€â”€ test_helpers.py                 # Installation routines, Brew/Winget, etc.
â”œâ”€â”€ test_plugin_manager.py          # Integration of OllamaManager / plugins
â”œâ”€â”€ test_service_manager.py         # Process control and status checking
â””â”€â”€ test_settings.py                # Configuration and JSON loading functions
```

<hr><br>

> The plugin consists of several specialized components:

- **OllamaManager:** Unified interface combining all modules into one facade.
- **OllamaAPIManager:** REST API communication through:
  `/api/tags`, `/api/show`, `/api/pull`, `/api/delete`, `/api/generate`, `/api/ps`
- **OllamaCMDManager:** Command-line execution:
  `ollama list`, `ollama show`, `ollama pull`, `ollama rm`, `ollama run`, `ollama ps`, `ollama stop`
- **OllamaService:** Service lifecycle management and health monitoring.
- **OllamaHelper:** Platform installers, validation, and utility functions.

<hr><br>

## âš–ï¸ Backend Comparison

| Feature | API Backend | CMD Backend |
|---------|------------|-------------|
| Performance | âš¡ Faster | â±ï¸ Slower |
| Streaming | âœ… Native | âœ… Line-based |
| Progress Info | âœ… Detailed | âš ï¸ Parsing required |
| Parameter Control | âœ… Full | âŒ Limited |
| Dependencies | requests | subprocess |

<hr><br>

## ğŸ’ Best Practices

- Use **API backend** for production or async apps
- Use **CMD backend** when the local API isnâ€™t reachable
- Run `health_check()` before critical operations
- Cache repeated generations for better performance
- Keep `verbose=True` during development for logs and printouts

<hr><br>

## ğŸ“œ License

This project is open source. See [License](LICENSE).

<hr><br>

## ğŸ™Œ Contributors

![created-by](docs/pictures/created-by.svg)

<hr><br>

## ğŸ§¯ Troubleshooting

### ğŸ§° Development Installation

If you encounter import issues (e.g. modules not found), try:

```bash
pip install -e .
```

This installs the project in **editable mode**, linking your local `src/` folder so that changes take effect immediately.

Then you can import directly:
```py
from modules.service_manager import Service
```

<hr><br>

## ğŸ’» Supported Platforms

- âœ… macOSÂ (Intelâ€¯&â€¯Appleâ€¯Silicon)
- âœ… LinuxÂ (Ubuntu, Debian, Fedoraâ€¦)
- âœ… WindowsÂ 10â€¯/â€¯11
